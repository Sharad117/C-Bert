{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('/kaggle/input/suicide-data-paired-for-contrastive-learning/suicide_data.csv')\n",
    "val_data=pd.read_csv('/kaggle/input/suicide-data-paired-for-contrastive-learning/test.csv').iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['class']=val_data['class'].map({'suicide':1,'non-suicide':0})\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embedding1, embedding2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(embedding1, embedding2)\n",
    "    loss = 0.5 * (label * distance.pow(2) + (1 - label) * F.relu(margin - distance).pow(2))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoding_latent(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(encoding_latent, self).__init__()\n",
    "        self.pre = model\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.pre.config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512,384 )\n",
    "        self.fc3 = nn.Linear(384,256)\n",
    "        # self.fc3 = nn.Linear(256, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pre_output = self.pre(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = pre_output.last_hidden_state[:,0,:]\n",
    "        \n",
    "        hidden = F.relu(self.fc1(embedding))  \n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = F.relu(self.fc2(hidden))\n",
    "        latent = F.relu(self.fc3(hidden))\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.text_column1 = 'anchor'\n",
    "        self.text_column2 = 'text'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1 = self.data.iloc[idx][self.text_column1]\n",
    "        text2 = self.data.iloc[idx][self.text_column2]\n",
    "        \n",
    "        inputs1 = self.tokenizer(text1, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        inputs2 = self.tokenizer(text2, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        label=self.data.iloc[idx]['label']\n",
    "        return inputs1, inputs2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= TextDataset('/kaggle/input/suicide-data-paired-for-contrastive-learning/suicide_data.csv',tokenizer)\n",
    "dataloader=DataLoader(dataset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c =encoding_latent(model)\n",
    "model_c=model_c.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=7\n",
    "opt=torch.optim.AdamW(model_c.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "    model_c.train()\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\"):\n",
    "        inputs1, inputs2, label = batch\n",
    "        label = label.to('cuda')\n",
    "        input_id1, attention_mask1 = inputs1['input_ids'].squeeze(1), inputs1['attention_mask'].squeeze(1)\n",
    "        input_id2, attention_mask2 = inputs2['input_ids'].squeeze(1), inputs2['attention_mask'].squeeze(1)\n",
    "        input_id1, attention_mask1 = input_id1.to('cuda'), attention_mask1.to('cuda')\n",
    "        input_id2, attention_mask2 = input_id2.to('cuda'), attention_mask2.to('cuda')\n",
    "        embedding1 = model_c(input_id1, attention_mask1)\n",
    "        embedding2 = model_c(input_id2, attention_mask2)\n",
    "        opt.zero_grad()\n",
    "        loss = contrastive_loss(embedding1, embedding2, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} --> Loss: {average_loss:.6f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_c,'vector_pooler_128.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
